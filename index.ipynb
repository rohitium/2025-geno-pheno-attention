{
 "cells": [
  {
   "cell_type": "raw",
   "id": "170215c2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"{{< var pub.title >}} --\"\n",
    "date: 'April 25, 2025'\n",
    "abstract-title: \"Summary\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2ab58-9efe-4758-a50d-8f9e011fe2cd",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "@Rijal2025 recently applied attention mechanisms to the problem of mapping genotype to phenotype. We noted their architecture omitted standard Transformer components, prompting us to test if including these elements could enhance performance on their multi-environment yeast dataset. Our analysis revealed that incorporating standard Transformer elements substantially improves predictive accuracy for this task.\n",
    "\n",
    "----\n",
    "\n",
    ":::{.callout-note title=\"AI usage disclosure\" collapse=\"true\"}\n",
    "This is a placeholder for the AI usage disclosure. Once all authors sign the AI code form on AirTable, SlackBot will message you an AI disclosure that you should place here.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6d953",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The recent preprint by @Rijal2025 introduces an application of attention mechanisms for inferring genotype-phenotype maps, particularly focusing on capturing complex epistatic interactions. They showed that their attention-based model outperformed linear and linear + pairwise models. Overall, their work sparked considerable interest and discussion within our journal club.\n",
    "\n",
    "While appreciating the novelty of applying attention in this domain, we noted that the specific architecture employed is relatively minimal. It utilizes stacked attention layers but omits several components commonly found in the original transformer model [@Vaswani2017], such as skip connections, layer normalization, or feed-forward blocks. We found these omissions interesting, since the transformer was the first architecture to fully leverage the power of attention mechanisms, and did so to great success in many distinct domains.\n",
    "\n",
    "This led us to a very straightforward inquiry: Could the performance of the attention-based genotype-phenotype model proposed by @Rijal2025 be improved by replacing it with a standard transformer architecture?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fb3f4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## The dataset\n",
    "\n",
    "The experimental data used in @Rijal2025 comes from the work of @Ba2022, who performed a large-scale quantitative trait locus (QTL) study in yeast. In short, they measured the growth rates of ~100,000 yeast segregants across 18 conditions and for ~40,000 loci, creating a massive dataset suitable for mapping genotype to phenotype.\n",
    "\n",
    "Due to extensive linkage disequilibrium (LD), the loci in the dataset are highly correlated with each other. To create a set of independent loci, @Rijal2025 a defined a set of loci such that the correlation between the SNPs present at any pair of loci is less than 94%, resulting in a set of 1164 \"independent\" loci.\n",
    "\n",
    "Unfortunately, they didn't provide this set of loci, nor the genotypic and phenotypic data used for training, so we located the [raw data](https://datadryad.org/dataset/doi:10.5061/dryad.1rn8pk0vd) that @Ba2022 originally uploaded alongside their study, then used [this notebook](https://github.com/Emergent-Behaviors-in-Biology/GenoPhenoMapAttention/blob/main/obtain_independent_loci.ipynb) uploaded by @Rijal2025 to recapitulate the 1164 loci. To save everyone else the trouble, we uploaded the train, test, and validation datasets we're *pretty sure* @Rijal2025 used in their study.\n",
    "\n",
    "::: {.callout-note}\n",
    "Roughly 2% of the phenotypic data are undefined (NaN).\n",
    ":::\n",
    "\n",
    "You can find the data here:\n",
    "\n",
    "TODO\n",
    "\n",
    "This data marks the start of our analysis, so we begin by downloading it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c4e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# TODO \n",
    "download_command_string = \"aws s3 sync s3://2025-attention-is-almost-all-you-need/inputs/ inputs/\"\n",
    "completed_process = subprocess.run(download_command_string.split(\" \"))\n",
    "assert completed_process.returncode == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577298a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboto3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01ms3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m S3Transfer\n\u001b[32m      2\u001b[39m S3Transfer(boto3.client(\u001b[33m\"\u001b[39m\u001b[33ms3\u001b[39m\u001b[33m\"\u001b[39m)).download_directory(\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2025-attention-is-almost-all-you-need\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs/\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'boto3'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf517d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "2025-geno-pheno-attention",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
