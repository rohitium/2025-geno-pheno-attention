{
  "hash": "b6d2e055e35620953dfd2ce18dbdb2e2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"{{< var pub.title >}} --\"\ndate: 'May 1, 2025'\ndate-modified: last-modified\nabstract-title: \"Summary\"\n---\n\n## Abstract\n\n@Rijal2025 recently applied attention mechanisms to the problem of mapping genotype to phenotype. We noted their architecture omitted standard transformer components, prompting us to test if including these elements could enhance performance on their multi-phenotype yeast dataset. Our analysis revealed that incorporating standard transformer elements substantially improves predictive accuracy for this task.\n\n----\n\n:::{.callout-note title=\"AI usage disclosure\" collapse=\"true\"}\nWe used Claude to help write code and clean up code. We used ChatGPT to help write code, clean up code, write text that we edited, suggest wording ideas and then chose which small phrases or sentence structure ideas to use, expand on summary text that we provided and then edited the text it produced, help clarify and streamline text that we wrote, interpret model training results data, and suggest papers on relevant science, we did further reading, and we cited some of this literature. We also provided ChatGPT with starting text and had it rearrange that text to fit the structure of one of our pub templates. We used Gemini in similar ways to help write code, clean up code, write text that we edited, suggest wording ideas and then chose which small phrases or sentence structure ideas to use, help clarify and streamline text that we wrote, interpret model training results data, and suggest papers on relevant science, we did further reading, and we cited some of this literature. We also provided Gemini with starting text and had it rearrange that text to fit the structure of one of our pub templates, and expanded on summary text that we provided and then edited the text it produced.\n:::\n\n## Introduction\n\nOn April 11th, @Rijal2025 released a preprint that introduces an application of attention mechanisms for inferring genotype-phenotype maps, particularly focusing on capturing complex epistatic interactions. This work sparked considerable interest given our ongoing exploration of nonlinear genotype-phenotype models.\n\nBy training on 100,000 yeast segregants over 18 growth phenotypes, @Rijal2025 demonstrated that an attention-based architecture extracts more epistatic signals than conventional linear methods, and does so with far fewer parameters than the full second-order regression that has become a norm in quantitative genetics. Their study therefore provides an important proof-of-principle for deep-learning architectures being able to cope with linkage disequilibrium, noise, and sparse high-order interactions in large-scale genotype-phenotype data.\n\nThat success naturally raised our curiosity. The network they used omits several standard transformer components that brought attention into the limelight: skip connections, layer normalisation, and feed-forward sub-layers [@Vaswani2017]. Given the cross-domain success that transformers have seen, we wondered: **Could the performance be further improved by replacing their model with a standard \"vanilla\" transformer architecture?** We found out. Also, along the way, we uncovered a missed opportunity by @Rijal2025 to reinforce the learning signal by properly leveraging cross-phenotype genetic correlations, which led to significant performance gains.\n\n::: {.callout-note icon=false}\n## {{< iconify fxemoji:lightningmood >}} Lightning fast shareable research\n\nWe offer this work not just as a technical add-on but as a proof of how open, fast-moving collaboration can accelerate discovery. @Rijal2025 shared their code and within twenty days of their preprint—and after only nine working days—we were able to validate, extend, and publicly release the next iteration. That cycle is orders of magnitude quicker than the traditional journal pipeline and illustrates the compounding value of doing bio ML in the open. We hope the present study sparks yet another round of improvements.\n:::\n\n## The dataset\n\nThe experimental data used in @Rijal2025 comes from the work of @Ba2022, who performed a large-scale quantitative trait locus (QTL) study in yeast. In short, they measured the growth rates of ~100,000 yeast segregants across 18 conditions and for ~40,000 loci, creating a massive dataset suitable for mapping genotype to phenotype.\n\nDue to extensive linkage disequilibrium (LD), the loci in the dataset are highly correlated with each other. To create a set of independent loci, @Rijal2025 defined a set of loci such that the correlation between the SNPs present at any pair of loci is less than 94%, resulting in a set of 1164 \"independent\" loci.\n\nWe were unable to find the set of loci, nor the genotypic and phenotypic data used for training, so we located the [raw data](https://datadryad.org/dataset/doi:10.5061/dryad.1rn8pk0vd) that @Ba2022 originally uploaded alongside their study, then used [this notebook](https://github.com/Emergent-Behaviors-in-Biology/GenoPhenoMapAttention/blob/main/obtain_independent_loci.ipynb) uploaded by @Rijal2025 to recapitulate the 1164 loci. To save everyone else the trouble, we uploaded the train, test, and validation datasets we're *pretty sure* @Rijal2025 used in their study.\n\nYou can find the data here: [https://zenodo.org/records/15313069](https://zenodo.org/records/15313069)\n\nWe'll use this data in what follows, so let's go ahead and download it into the current working directory:\n\n::: {#81c4e7b1 .cell execution_count=1}\n``` {.python .cell-code}\nimport subprocess\nfrom pathlib import Path\n\ndataset_dir = Path(\"datasets/\")\nif not dataset_dir.exists():\n    zenodo_url = \"https://zenodo.org/records/15313069/files/datasets.zip\"\n    subprocess.run([\"wget\", zenodo_url])\n    subprocess.run([\"unzip\", \"datasets.zip\"])\n    Path(\"datasets.zip\").unlink()\n```\n:::\n\n\n::: {#8d4b415c .cell .column-margin tbl-cap='Table 1. The test dataset $R^2$ values for the attention model in Figure 3 of Rijal et al. Calculated by manual pixel counting.' execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Phenotype</th>\n      <th>Source R²</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23C</td>\n      <td>0.612</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25C</td>\n      <td>0.621</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27C</td>\n      <td>0.632</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30C</td>\n      <td>0.609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33C</td>\n      <td>0.615</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>35C</td>\n      <td>0.582</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>37C</td>\n      <td>0.566</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>cu</td>\n      <td>0.610</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>suloc</td>\n      <td>0.638</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ynb</td>\n      <td>0.487</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>eth</td>\n      <td>0.604</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>gu</td>\n      <td>0.566</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>li</td>\n      <td>0.630</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>mann</td>\n      <td>0.600</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>mol</td>\n      <td>0.618</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>raff</td>\n      <td>0.653</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>sds</td>\n      <td>0.630</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4NQO</td>\n      <td>0.609</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Reproducing single phenotype results\n\nLet's first try and reproduce the single-phenotype attention model performances observed in Figure 3 (red dots). This will give us the confidence to know we both **(a)** correctly reverse-engineered the specifics of their training/validation/test datasets and **(b)** accurately implemented their model.\n\n![Figure 3 from @Rijal2025. Original caption: \"*Comparison of model performance in yeast QTL mapping data. We show R² on test datasets for linear, linear+pairwise, and attention-based model (with d = 12) across 18 phenotypes (relative growth rates in various environments). For the linear + pairwise mode, the causal loci inferred by @Ba2022 are used.*\"](assets/fig3.jpg){fig-align=\"center\" width=100% fig-alt=\"Figure 3 from @Rijal2025 showing the single-environment model performances.\"}\n\nNote that a separate model is trained on each phenotype, so reproducing this figure involves training 18 models. To do this, we need to create config objects specifying each model architecture and how the training should proceed.\n\n::: {#27f1bc0f .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport attrs\nfrom analysis.base import ModelConfig, TrainConfig\nfrom analysis.dataset import phenotype_names\n\nmodel_config = ModelConfig(\n    model_type=\"rijal_et_al\",\n    seq_length=1164,\n    embedding_dim=13,\n    num_layers=3,\n)\n\n# This template config sets all the shared parameters.\ntrain_template_config = TrainConfig(\n    data_dir=dataset_dir,\n    save_dir=Path(\"models/fig3\"),\n    name_prefix=\"fig3_23C\",\n    phenotypes=[\"23C\"],\n    optimizer=\"adam\",\n    batch_size=64,\n    learning_rate=0.001,\n    lr_schedule=False,\n    weight_decay=0.0,\n    max_epochs=200,\n    gradient_clip_val=0,\n    use_cache=True,\n    use_modal=False,\n)\n\njobs = []\nfor phenotype in phenotype_names:\n    # Each train config needs to set its corresponding phenotype(s).\n    phenotype_config = attrs.evolve(\n        train_template_config, phenotypes=[phenotype], name_prefix=f\"fig3_{phenotype}\"\n    )\n    jobs.append((model_config, phenotype_config))\n```\n:::\n\n\nWith each job properly configured, we can train a model for each phenotype:\n\n::: {#1b8c4b90 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nfrom analysis.train import run_trainings\n\nmodel_dirs = run_trainings(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-trained model 'fig3_23C' found. Returning path.\nPre-trained model 'fig3_25C' found. Returning path.\nPre-trained model 'fig3_27C' found. Returning path.\nPre-trained model 'fig3_30C' found. Returning path.\nPre-trained model 'fig3_33C' found. Returning path.\nPre-trained model 'fig3_35C' found. Returning path.\nPre-trained model 'fig3_37C' found. Returning path.\nPre-trained model 'fig3_cu' found. Returning path.\nPre-trained model 'fig3_suloc' found. Returning path.\nPre-trained model 'fig3_ynb' found. Returning path.\nPre-trained model 'fig3_eth' found. Returning path.\nPre-trained model 'fig3_gu' found. Returning path.\nPre-trained model 'fig3_li' found. Returning path.\nPre-trained model 'fig3_mann' found. Returning path.\nPre-trained model 'fig3_mol' found. Returning path.\nPre-trained model 'fig3_raff' found. Returning path.\nPre-trained model 'fig3_sds' found. Returning path.\nPre-trained model 'fig3_4NQO' found. Returning path.\n```\n:::\n:::\n\n\n::: {.callout-important appearance=\"simple\"}\n# A note about training behavior\nThe above code will initiate training for all configured jobs, with important considerations:\n\n* Caching: Since `train_config.use_cache = True` and these models are stored in the GitHub repository, executing this locally will use cached models instead of performing expensive retraining\n* Training Duration: Each training job takes approximately 2.5 hours to complete on an A10G GPU, so running all jobs without caching would require significant time\n* Compute Configuration: In our production environment, we set `train_config.use_modal = True` to distribute compute jobs via [Modal](https://modal.com/). For compatibility with different compute architectures, this notebook uses `train_config.use_modal = False` by default.\n:::\n\nInside each run directory is a `metrics.csv` that we can get the test dataset $R^2$ from and compare directly against Figure 3 from @Rijal2025.\n\n::: {#cell-fig-rep .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nfrom pathlib import Path\n\nimport arcadia_pycolor as apc\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\napc.mpl.setup()\n\n\ndef get_test_r2(model_dir: Path) -> float:\n    metrics = pd.read_csv(model_dir / \"metrics.csv\")\n    return float(metrics.loc[metrics[\"metric\"] == \"test_r2\", \"value\"].iloc[0])\n\n\nfig3_reproduction_r2 = [get_test_r2(d) for d in model_dirs]\nfig3_results[\"reproduction R2\"] = fig3_reproduction_r2\ndf = pd.DataFrame(fig3_results)\n\nx = np.arange(len(df))\nwidth = 0.35\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    plt.bar(x - width / 2, df[\"source R2\"], width, label=\"Source R²\", color=apc.cloud)\n    plt.bar(x + width / 2, df[\"reproduction R2\"], width, label=\"Reproduction R²\", color=apc.steel)\n\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"R² Score\")\n    plt.ylim(0.46, 0.67)\n    plt.xticks(x, df[\"phenotype\"], rotation=90)\n    plt.legend(loc=(0.05, 0.90), ncol=2)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Comparison of test-set $R^2$ values between @Rijal2025 (“Source”) and our re-implementation (\"Reproduction\").](index_files/figure-html/fig-rep-output-1.png){#fig-rep}\n:::\n:::\n\n\nWith an $n$ of 1, we can't assess reproducibility rigorously. Even so, our re-implementation matches the published numbers very closely. Here are some numbers on the phenotype with the largest deviation:\n\n::: {#9d96d37f .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ndeltas = (df[\"source R2\"] - df[\"reproduction R2\"]).abs()\nmax_idx = deltas.argmax()\nmax_delta = deltas.max()\nphenotype_with_largest_delta = df[\"phenotype\"].iloc[max_idx]\npercent_diff = 100 * max_delta / df[\"source R2\"].iloc[max_idx]\n\nprint(\"Biggest discrepenacy:\")\nprint(f\"- Phenotype: {phenotype_with_largest_delta}\")\nprint(f\"- R2 difference: {max_delta:.3f}\")\nprint(f\"- Percent difference: {percent_diff:.1f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBiggest discrepenacy:\n- Phenotype: 33C\n- R2 difference: 0.008\n- Percent difference: 1.3\n```\n:::\n:::\n\n\nOverall, these results help assure us that:\n\n1. The train/validation/test partitions are identical, or at least functionally equivalent\n2. Our code reproduces the authors' architecture and training procedure.\n\nWith this validated baseline in place, we're now confident in using it as the starting point for modifying the architecture.\n\n## Canonical ML components outperform bespoke customization\n\nThe @Rijal2025 model architecture uses non-standard components. To test whether these idiosyncratic choices are actually helpful, we replaced each one with a standard, \"textbook\" alternative and measured the collective impact on predictive accuracy.\n\n| Non-standard element in Rijal *et al.* | What it does | Canonical replacement we tried |\n|---------------------------------------|--------------|--------------------------------|\n| **Concatenated column of ones** | Appends a constant 1 to every token to mimic an explicit bias term (Appendix F in the original paper). | Simply enable the bias in the linear Q, K, V projections (`nn.Linear(..., bias=True)`). |\n| **Phenotype represented as an *input* token** | Adds one-hot phenotype tokens to the sequence, forcing attention layers to discover gene × phenotype interactions. | Condition the model *after* the attention block: predict all phenotypes jointly from a pooled sequence representation (see below). |\n| **Flattened fitness matrix** | Treats each (genotype, phenotype) pair as an independent sample; the network outputs one scalar at a time. | Mean-pool the token embeddings → `(B, D)` and use a single linear layer `D → 18` so all phenotypes are predicted simultaneously. |\n\n### Why predict all phenotypes at once?\n\nThe measured phenotypes are likely to be genetically correlated due to biological processes such as pleiotropy. Consequently, knowing that a mutation hurts growth in one condition has the potential to inform its effect in another. A shared output head lets the network exploit this mutual information, whereas the original set-up can only share knowledge through the shared attention weights. Our phenotype-phenotype autoencoder study [@Avasthi2023] showed significant prediction benefits when accounting for phenotypic covariation, so we’re expecting the same thing to be true here, too.\n\n### Experimental protocol\n\nThe codebase contains a new architecture with the above modifications. Let's test its performance with the following changes to the training:\n\n* Increase the number of phenotypes from $1$ to $18$ (all).\n* Correspondingly increase the hidden dimension from $d=12$ to $d=128$.\n* Decrease the learning rate 10-fold, from $1 \\times 10^{-3}$ to $1 \\times 10^{-4}$.\n\nFinally, because @fig-rep hints at potentially significant run-to-run variance, let's run five replicates, where each replicate differs only in its initializing seed (the train/validation/test splits are held constant).\n\n::: {#3ac9eb98 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"std_d128_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=128,\n        model_type=\"modified\",\n        seq_length=1164,\n        num_layers=3,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/canonical\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=100,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerated 5 job configurations.\n```\n:::\n:::\n\n\n::: {.callout-note}\nIf you're running this yourself and aren't interested in replicates, you can reduce the amount of required compute by setting `NUM_REPLICATES` in the above cell to 1.\n:::\n\nNow, let's run the experiment:\n\n::: {#b2e6f770 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nmodel_dirs = run_trainings(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-trained model 'std_d128_rep_00' found. Returning path.\nPre-trained model 'std_d128_rep_01' found. Returning path.\nPre-trained model 'std_d128_rep_02' found. Returning path.\nPre-trained model 'std_d128_rep_03' found. Returning path.\nPre-trained model 'std_d128_rep_04' found. Returning path.\n```\n:::\n:::\n\n\n## Consistent gains across phenotypes \n\n@fig-std shows that swapping the custom choices in @Rijal2025 for a canonical *embedding $\\rightarrow$ mean-pool $\\rightarrow$ linear-head* model and predicting all 18 phenotypes jointly yields a boost in predictive power across the board:\n\n::: {#cell-fig-std .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nlabel_lookup = {\n    \"std\": \"Canonical\",\n}\n\n\ndef get_phenotype_r2_data(model_dir: Path) -> float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    base_arch_key, _, _ = variant_key.rpartition(\"_d\")\n    architecture_label = label_lookup[base_arch_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ncanonical_plot_data = pd.concat([get_phenotype_r2_data(model_dir) for model_dir in model_dirs])\n\n# Calculate the mean and standard error R2 over replicates\ncanonical_plot_data = (\n    canonical_plot_data.groupby([\"architecture\", \"phenotype\"])[\"r2\"]\n    .agg(mean_r2=\"mean\", std_r2=\"std\")\n    .reset_index()\n)\n\n# Concatenate the manually scraped Fig3 R2s\nrijal_r2s = fig3_results.copy(deep=True)\nrijal_r2s.drop(\"reproduction R2\", axis=1, inplace=True)\nrijal_r2s.rename(columns={\"source R2\": \"mean_r2\"}, inplace=True)\nrijal_r2s[\"std_r2\"] = 0.0\nrijal_r2s[\"architecture\"] = \"Rijal\"\ncanonical_plot_data = pd.concat([canonical_plot_data, rijal_r2s])\n\n# Plotting code.\ncanonical_plot_data[\"phenotype\"] = pd.Categorical(\n    canonical_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            canonical_plot_data[canonical_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 0.5) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Per-phenotype generalisation of multi-task vs. single-task models. Bar heights show the mean test-set $R^{2}$ achieved across replicates for each phenotype, with error bars denoting the standard deviation (SD). \"Rijal\" is the single-phenotype results reported in @Rijal2025, while \"Canonical\" is our multi-output head trained to predict all phenotypes jointly.](index_files/figure-html/fig-std-output-1.png){#fig-std}\n:::\n:::\n\n\n::: {.callout-important appearance=\"simple}\n### Important caveat on the baseline we compare against\n\nThe numbers labeled \"Rijal\" in @fig-std come from the single-phenotype models reported in their Figure 3. @Rijal2025 do develop a multi-phenotype architecture, but careful inspection of [their code](https://github.com/Emergent-Behaviors-in-Biology/GenoPhenoMapAttention/blob/main/experiment/multi_env_attention_QTL_yeast_data.ipynb) shows that it still emits a scalar output per forward pass and simply loops over phenotypes at train time. In other words, each (genotype, phenotype) pair is treated as an independent sample. The predictive power of this model is shown in their Figure 4, and is actually worse than the single-phenotype models at predicting any given phenotype.\n\nBecause our goal is to test whether a genuine multi-output head + canonical components confer an advantage, we chose the best-performing baseline available (their single-phenotype models). The comparison is therefore conservative: any gains we report would be larger, not smaller, if the authors' scalar multi-phenotype model were used as the reference.\n:::\n\nQuantifying which changes contributed to these gains would require an ablation study, however our hunch is that the primary gains come from cross-phenotype genetic correlations. For instance we note that some of the most consistent gains we see in our implementation of the model come from fitness measurements at different temperatures, a set of phenotypes that is almost certainly impacted by pleiotropic sets of genes. Joint training allows the network to transfer information among genetically correlated traits, an advantage that the single-output baseline can't exploit beyond implicit relationships learned through shared attention weights.\n\nTaken together, these results validate the canonical multi-output model as a stronger starting point than the original design.\n\n## A vanilla transformer architecture\n\nThe current architecture still departs from the reference transformer in several respects: it omits residual connections, layer normalization, and position-wise feed-forward blocks. The logical next experiment is therefore to **level up to a bona-fide vanilla transformer**, preserving the current tokenization while adding:\n\n1. **Residual (skip) connections** and **pre-LayerNorm**,  \n2. **Feed-forward sub-layers** with RELU activations,  \n3. **Scaled dot-product attention** with the canonical $1/\\sqrt{d}$ factor,  \n4. **Dropout** and **weight-decay** for regularization to prevent overfitting.\n\nIt's worth noting that transformers excel at sequence modeling---and sequences are ordered collections. While the loci in this dataset could be attributed chromosomal coordinates, and therefore in a sense an ordering, like @Rijal2025, we're treating the loci as an unordered collection. Thus, we don't add any positional encodings, either in the form of absolute or relative positional encodings.\n\n::: {#112687cf .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"xformer_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=256,\n        model_type=\"transformer\",\n        seq_length=1164,\n        num_layers=3,\n        nhead=4,\n        dim_feedforward=1024,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/transformer\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=80,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerated 5 job configurations.\n```\n:::\n:::\n\n\n::: {#f19ffd48 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nmodel_dirs = run_trainings(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-trained model 'xformer_rep_00' found. Returning path.\nPre-trained model 'xformer_rep_01' found. Returning path.\nPre-trained model 'xformer_rep_02' found. Returning path.\nPre-trained model 'xformer_rep_03' found. Returning path.\nPre-trained model 'xformer_rep_04' found. Returning path.\n```\n:::\n:::\n\n\n@fig-xformer shows the change in performance:\n\n::: {#cell-fig-xformer .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nlabel_lookup = {\n    \"std\": \"Canonical\",\n    \"xformer\": \"Transformer\",\n}\n\n\ndef get_phenotype_r2_data_xformer(model_dir: Path) -> float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    architecture_label = label_lookup[variant_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ndf = pd.concat([get_phenotype_r2_data_xformer(model_dir) for model_dir in model_dirs])\ndf = df.groupby([\"architecture\", \"phenotype\"])[\"r2\"].agg(mean_r2=\"mean\", std_r2=\"std\").reset_index()\n\n# Concat with existing canonical plot data\nxformer_plot_data = pd.concat([df, canonical_plot_data])\n\n# Plotting code.\nxformer_plot_data[\"phenotype\"] = pd.Categorical(\n    xformer_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\", \"Transformer\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n    \"Transformer\": apc.dragon,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            xformer_plot_data[xformer_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 1) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The vanilla transformer performances added alongside the models compared in @fig-std.](index_files/figure-html/fig-xformer-output-1.png){#fig-xformer}\n:::\n:::\n\n\nThe full transformer nudges performance above the already-improved canonical model across most phenotypes (@fig-xformer). The lift is apparent, but no stepwise performance gains are observed. There's undoubtedly a lot of room for improvement: we made no attempt at hyper-parameter tuning beyond some cursory investigations of dropout and weight-decay (neither of which positively affected performance), and all other parameters were guessed zero shot.\n\n## Conclusion and outlook  \n\nOur central question was simple: *do off-the-shelf transformer components---and a genuine multi-output head---move the needle on genotype to phenotype prediction?*  \n\nThe answer is an emphatic **yes**.\n\n### Why multi-output matters  \n\nLeveraging mutual information between genetically correlated phenotypes represents a natural way of boosting model performance when training data is limited. In the context of plant/animal breeding, multi-trait genomic prediction has been well established to improve the predictive power of linear models (particularly for low heritability traits) [@Jia2012]. Our own work with phenotype-phenotype autoencoders has demonstrated that encoding multiple phenotypes jointly lets an auto-encoder predict individual phenotypes with very high accuracy, particularly as the number of phenotypes considered simultaneously increases [@Avasthi2023]. These observations are examples of the benefits of multi-task learning, which have long been appreciated in ML literature [@Caruana1997]. Our presented results are no different: when the network can see all 18 phenotypes at once, shared genetic effects (either through pleiotropy or in some instances through LD [@Jobran2021]) reinforce, rather than fragment, the learning signal.\n\n### A foundation, not a finish line  \n\nA natural next step given these results is to test cross-environment transfer learning in our multi-task model. @Rijal2025 demonstrated that transfer learning was possible across temperature growth conditions even with a multi-environment model that did not outperform a single-environment model. Given the overall superior performance of the models we present here, they may be particularly well suited for fitness prediction for novel phenotypes with even fewer fine-tuning observations than used by @Rijal2025.\n\nFinally, it would be interesting to evaluate where the performance boost of multi-task learning comes from, from a quantitative genetics perspective. Phenotypic variance can generally be broken down into additive ($G$), epistatic ($G \\times G$) and gene by environment ($G \\times E$) components (ignoring higher order terms for simplicity). Intuitively, we might expect multi-trait learning to excel at explaining $G$ and $G \\times G$ variance components that are constant across environments, however it’s possible that $G \\times E$ variance is also better explained, as multi-task learning benefits are known to extend to even unrelated tasks [@Paredes2012]. We suspect this could be investigated through ablation studies that track environment specific prediction outcomes.\n\nThere is also still plenty of headroom for improving the model:\n\n* No hyper-parameter search was attempted for the transformer; depth, head count, LR schedules, dropout rates, and layer widths all remain untouched.\n* How far can the canonical model be pushed? Maybe the transformer is overkill.\n* The model treats the loci as an unordered collection. It’s possible that adding chromosomal coordinates or some kind of other positional encoding could let the model sing.\n* We haven’t experimented with more sophisticated feature selection methods for reducing the number of loci prior to model training, a task that may be particularly fruitful for improving the ability of models to capture pairwise and higher order epistasis.\n\n### An invitation to build further  \n\nAll code, configs, and cached model checkpoints are available in this notebook’s [repository](https://github.com/Arcadia-Science/2025-geno-pheno-attention). The Appendix documents the engineering decisions that should help orient you to the codebase.\n\n## Appendix: a codebase primer\n\n::: {.callout-note}\nThis section elaborates on engineering details that will be of interest to those planning to reproduce, modify, or build off of this research.\n:::\n\nRather than building off the @Rijal2025 notebook files, we re-implemented their code into our own codebase to improve code quality and make room for our modifications and experimentation. Here is a very high-level summary of the changes we made:\n\n* Added configuration dataclasses to co-localize tunables\n* Saved the training/validation/test datasets to file to avoid splitting on-the-fly\n* Created PyTorch dataloaders to manage accession, batching, and data shuffling\n* Automated the training loop with [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), creating separation between the training loop and the model logic\n* Added canonical learning parameters like early stopping, learning rate scheduling, gradient norm clipping, weight decay, and more\n* Generalized the @Rijal2025 model with toggleable skip connections, layer normalization, scaled attention, dropout rate, and more\n\nThe upshot is that we're proud of this code and think it establishes a much-needed foundation that can be used to build off the research seeded by @Rijal2025.\n\n### Training models\n\nIn the analysis above, we illustrated how multiple training jobs can be run using the high level entrypoint, `run_trainings`, that:\n\n* Trains a model for a given set of phenotypes\n* Determines the best model, defined as the model with the lowest loss (MSE) calculated over the validation dataset\n* Reports the $R^2$ for the test dataset using the best model\n* Saves the model to file for downstream use\n\nThe codebase also exposes equivalent behavior through a command-line interface (CLI). It can be accessed via:\n\n```shell\n$ python -m analysis.train --help\n```\n\n### Distributed computing\n\nWe performed the analysis using [Modal](https://modal.com/)’s cloud infrastructure to distribute computations across GPUs, allowing us to rapidly measure performance across many different model architectures and training specifications. Whether you want to train with or without Modal can be toggled by the attribute `train.use_modal`. By default, Modal execution is disabled. The downside is that your training jobs will run in serial, rather than being distributed across different machines.\n\n### Caching behavior\n\nWe implemented a simple cache mechanism that avoids training if a model directory for a given training config already exists. We did this so that GPUs aren't a requirement for engaging with this research.\n\n* `train_config.use_cache = True` (default): Skips retraining if a model with the same configuration already exists\n* `train_config.use_cache = False`: Forces retraining regardless of existing models\n\n::: {.callout-note}\nAll training runs in this analysis use the default caching mode (`train_config.use_cache = True`), and the results are git tracked. As a result, if you execute this notebook locally, these models will be loaded from cache rather than retrained.\n:::\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}