{
  "hash": "29599550213f3f4bda4a76bba9e1efc2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"{{< var pub.title >}} --\"\ndate: 'April 30, 2025'\nabstract-title: \"Summary\"\n---\n\n## Abstract\n\n@Rijal2025 recently applied attention mechanisms to the problem of mapping genotype to phenotype. We noted their architecture omitted standard transformer components, prompting us to test if including these elements could enhance performance on their multi-phenotype yeast dataset. Our analysis revealed that incorporating standard transformer elements substantially improves predictive accuracy for this task.\n\n----\n\n:::{.callout-note title=\"AI usage disclosure\" collapse=\"true\"}\nThis is a placeholder for the AI usage disclosure. Once all authors sign the AI code form on AirTable, SlackBot will message you an AI disclosure that you should place here.\n:::\n\n## Introduction\n\nThe recent preprint by @Rijal2025 introduces an application of attention mechanisms for inferring genotype-phenotype maps, particularly focusing on capturing complex epistatic interactions. They showed that their attention-based model outperformed linear and linear + pairwise models. Overall, their work sparked considerable interest and discussion within our journal club.\n\nWhile appreciating the novelty of applying attention in this domain, we noted that the specific architecture employed is non-canonical. It utilizes stacked attention layers but omits several components commonly found in the original transformer model [@Vaswani2017], such as skip connections, layer normalization, or feed-forward blocks. We found these omissions interesting, since the transformer was the first architecture to fully leverage the power of attention mechanisms, and did so to great success in many distinct domains. Additionally, the architecture includes several bespoke components with lengthy mathematical justifications.\n\nThis led us to a very **straightforward inquiry**: Could the performance of the attention-based genotype-phenotype model proposed by @Rijal2025 be improved by replacing it with a standard \"vanilla\" transformer architecture?\n\n## The dataset\n\nThe experimental data used in @Rijal2025 comes from the work of @Ba2022, who performed a large-scale quantitative trait locus (QTL) study in yeast. In short, they measured the growth rates of ~100,000 yeast segregants across 18 conditions and for ~40,000 loci, creating a massive dataset suitable for mapping genotype to phenotype.\n\nDue to extensive linkage disequilibrium (LD), the loci in the dataset are highly correlated with each other. To create a set of independent loci, @Rijal2025 a defined a set of loci such that the correlation between the SNPs present at any pair of loci is less than 94%, resulting in a set of 1164 \"independent\" loci.\n\nUnfortunately, they didn't provide this set of loci, nor the genotypic and phenotypic data used for training, so we located the [raw data](https://datadryad.org/dataset/doi:10.5061/dryad.1rn8pk0vd) that @Ba2022 originally uploaded alongside their study, then used [this notebook](https://github.com/Emergent-Behaviors-in-Biology/GenoPhenoMapAttention/blob/main/obtain_independent_loci.ipynb) uploaded by @Rijal2025 to recapitulate the 1164 loci. To save everyone else the trouble, we uploaded the train, test, and validation datasets we're *pretty sure* @Rijal2025 used in their study.\n\nYou can find the data here: [https://zenodo.org/records/15313069](https://zenodo.org/records/15313069)\n\nWe'll use this data in what follows, so let's go ahead and download it into the current working directory:\n\n::: {#81c4e7b1 .cell execution_count=1}\n``` {.python .cell-code}\nimport subprocess\nfrom pathlib import Path\n\ndataset_dir = Path(\"datasets/\")\nif not dataset_dir.exists():\n    zenodo_url = \"https://zenodo.org/records/15313069/files/datasets.zip\"\n    subprocess.run([\"wget\", zenodo_url])\n    subprocess.run([\"unzip\", \"datasets.zip\"])\n    Path(\"datasets.zip\").unlink()\n```\n:::\n\n\n::: {#8d4b415c .cell .column-margin tbl-cap='Table 1. The test dataset $R^2$ values for the attention model in Figure 3 of @Rijal2025. Calculated by manual pixel counting.' execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>phenotype</th>\n      <th>source R2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23C</td>\n      <td>0.612</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25C</td>\n      <td>0.621</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>27C</td>\n      <td>0.632</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30C</td>\n      <td>0.609</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33C</td>\n      <td>0.615</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>35C</td>\n      <td>0.582</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>37C</td>\n      <td>0.566</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>cu</td>\n      <td>0.610</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>suloc</td>\n      <td>0.638</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ynb</td>\n      <td>0.487</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>eth</td>\n      <td>0.604</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>gu</td>\n      <td>0.566</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>li</td>\n      <td>0.630</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>mann</td>\n      <td>0.600</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>mol</td>\n      <td>0.618</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>raff</td>\n      <td>0.653</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>sds</td>\n      <td>0.630</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4NQO</td>\n      <td>0.609</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Reproducing single phenotype results\n\nLet's first try and reproduce the single-phenotype attention model performances observed in Figure 3 (red dots). This will give us to the confidence to know we correctly both **(a)** reverse-engineered the specifics of their training/validation/test datasets and **(b)** accurately implemented their model.\n\n![Figure 3 from @Rijal2025. Original caption: \"*Comparison of model performance in yeast QTL mapping data. We show R2 on test datasets for linear, linear+pairwise, and attention-based model (with d = 12) across 18 phenotypes (relative growth rates in various environments). For the linear + pairwise mode, the causal loci inferred by @Ba2022 are used.*\"](assets/fig3.jpg){fig-align=\"center\" width=100% fig-alt=\"Figure 3 from @Rijal2025 showing the single-environment model performances.\"}\n\nNote that a separate model is trained on each phenotype, so reproducing this figure involves training 18 models. To do this, we need to create config objects specifying each model architecture and how the training should proceed.\n\n::: {#27f1bc0f .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport attrs\nfrom analysis.base import ModelConfig, TrainConfig\nfrom analysis.dataset import phenotype_names\n\nmodel_config = ModelConfig(\n    model_type=\"rijal_et_al\",\n    seq_length=1164,\n    embedding_dim=13,\n    num_layers=3,\n)\n\n# This template config sets all the shared parameters.\ntrain_template_config = TrainConfig(\n    data_dir=dataset_dir,\n    save_dir=Path(\"models/fig3\"),\n    name_prefix=\"fig3_23C\",\n    phenotypes=[\"23C\"],\n    optimizer=\"adam\",\n    batch_size=64,\n    learning_rate=0.001,\n    lr_schedule=False,\n    weight_decay=0.0,\n    max_epochs=200,\n    gradient_clip_val=0,\n    use_cache=True,\n    use_modal=False,\n)\n\njobs = []\nfor phenotype in phenotype_names:\n    # Each train config needs to set its corresponding phenotype(s).\n    phenotype_config = attrs.evolve(\n        train_template_config, phenotypes=[phenotype], name_prefix=f\"fig3_{phenotype}\"\n    )\n    jobs.append((model_config, phenotype_config))\n```\n:::\n\n\nWith each job properly configured, we can train a model for each phenotype:\n\n::: {#1b8c4b90 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nfrom analysis.train import run_trainings\n\nmodel_dirs = run_trainings(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-trained model 'fig3_23C' found. Returning path.\nPre-trained model 'fig3_25C' found. Returning path.\nPre-trained model 'fig3_27C' found. Returning path.\nPre-trained model 'fig3_30C' found. Returning path.\nPre-trained model 'fig3_33C' found. Returning path.\nPre-trained model 'fig3_35C' found. Returning path.\nPre-trained model 'fig3_37C' found. Returning path.\nPre-trained model 'fig3_cu' found. Returning path.\nPre-trained model 'fig3_suloc' found. Returning path.\nPre-trained model 'fig3_ynb' found. Returning path.\nPre-trained model 'fig3_eth' found. Returning path.\nPre-trained model 'fig3_gu' found. Returning path.\nPre-trained model 'fig3_li' found. Returning path.\nPre-trained model 'fig3_mann' found. Returning path.\nPre-trained model 'fig3_mol' found. Returning path.\nPre-trained model 'fig3_raff' found. Returning path.\nPre-trained model 'fig3_sds' found. Returning path.\nPre-trained model 'fig3_4NQO' found. Returning path.\n```\n:::\n:::\n\n\n::: {.callout-important appearance=\"simple\"}\n# A note about training behavior\nThe above code will initiate training for all configured jobs, with important considerations:\n\n* Caching: Since `train_config.use_cache = True` and these models are stored in the GitHub repository, executing this locally will use cached models instead of performing expensive retraining\n* Training Duration: Each training job takes approximately 2.5 hours to complete on an A10G GPU, so running all jobs without caching would require significant time\n* Compute Configuration: In our production environment, we set `train_config.use_modal = True` to distribute compute jobs via [Modal](https://modal.com/). For compatibility with different compute architectures, this notebook uses `train_config.use_modal = False` by default.\n:::\n\nInside each run directory is a `metrics.csv` that we can get the test dataset $R^2$ from and compare directly against Figure 3.\n\n::: {#cell-fig-reproduction .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nfrom pathlib import Path\n\nimport arcadia_pycolor as apc\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\napc.mpl.setup()\n\n\ndef get_test_r2(model_dir: Path) -> float:\n    metrics = pd.read_csv(model_dir / \"metrics.csv\")\n    return float(metrics.loc[metrics[\"metric\"] == \"test_r2\", \"value\"].iloc[0])\n\n\nfig3_reproduction_r2 = [get_test_r2(d) for d in model_dirs]\nfig3_results[\"reproduction R2\"] = fig3_reproduction_r2\ndf = pd.DataFrame(fig3_results)\n\nx = np.arange(len(df))\nwidth = 0.35\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    plt.bar(x - width / 2, df[\"source R2\"], width, label=\"Source R²\", color=apc.cloud)\n    plt.bar(x + width / 2, df[\"reproduction R2\"], width, label=\"Reproduction R²\", color=apc.steel)\n\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"R² Score\")\n    plt.ylim(0.46, 0.67)\n    plt.xticks(x, df[\"phenotype\"], rotation=90)\n    plt.legend(loc=(0.05, 0.90), ncol=2)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Comparison of test-set $R^2$ values between @Rijal2025 (“Source”) and our re-implementation (\"Reproduction\").](index_files/figure-html/fig-reproduction-output-1.png){#fig-reproduction}\n:::\n:::\n\n\nWith an $n$ of 1, we can't assess reproducibility rigorously. Even so, our re-implementation matches the published numbers very closely: the largest gap--​for phenotype **mol**--is an $R^2$ difference of just 0.014, i.e. 2.2% relative error.\n\nThese results helped assure us that\n\n1. The train/validation/test partitions are identical, or at least functionally equivalent\n2. Our code reproduces the authors' architecture and training procedure.\n\nWith this validated baseline in place, we're now confident in using it as the starting point for modifying the architecture.\n\n# Canonical ML components outperform bespoke tricks\n\nThe @Rijal2025 model architecture uses non-standard components. To test whether these idiosyncratic choices are actually helpful, we replaced each one with a standard, \"textbook\" alternative and measured the impact on predictive accuracy.\n\n| Non-standard element in Rijal *et al.* | What it does | Canonical replacement we tried |\n|---------------------------------------|--------------|--------------------------------|\n| **Random projection of a diagonal genotype matrix** `X(g)·R` | Encodes each locus as a row of a fixed random matrix `R`, multiplied by the allele sign (±1). | A learned embedding table `nn.Embedding(L, D)` whose rows are multiplied by the allele sign. |\n| **Concatenated column of ones** | Appends a constant 1 to every token to mimic an explicit bias term (Appendix F in the original paper). | Simply enable the bias in the linear Q, K, V projections (`nn.Linear(..., bias=True)`). |\n| **Phenotype represented as an *input* token** | Adds one-hot phenotype tokens to the sequence, forcing attention layers to discover gene × phenotype interactions. | Condition the model *after* the attention block: predict all phenotypes jointly from a pooled sequence representation (see below). |\n| **Flattened fitness matrix** | Treats each (genotype, phenotype) pair as an independent sample; the network outputs one scalar at a time. | Mean-pool the token embeddings → `(B, D)` and use a single linear layer `D → 18` so all phenotypes are predicted simultaneously. |\n\n### Why predict all phenotypes at once?\n\nBiologically, the phenotypes are pleiotropically linked; knowing that a mutation hurts growth in one condition has the potential to inform its affect at another. A shared output head lets the network exploit that mutual information, whereas the original set-up can only share knowledge through the shared attention weights. We're expecting this relationship between phenotypes to reinforce each other and lead to better model performance.\n\n### Experimental protocol\n\nThe codebase contains a new architecture with the above modifications. Let's test its performance with the following changes to the training:\n\n* Increase the number of phenotypes from $1$ to $18$ (all)\n* Correspondingly increase the hidden dimension from $d=12$ to $d=128$.\n* Decrease the learning rate 10-fold, from $1 \\times 10^{-3}$ to $1 \\times 10^{-4}$.\n\nFinally, because @fig-reproduction hints at potentially significant run-to-run variance, let's run five replicates, where each replicate differs only in its initializing seed (the train/validation/test splits are held constant).\n\n::: {#3ac9eb98 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"std_d128_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=128,\n        model_type=\"modified\",\n        seq_length=1164,\n        num_layers=3,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/canonical\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=100,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerated 5 job configurations.\n```\n:::\n:::\n\n\n::: {.callout-note}\nIf you're running this yourself and aren't interested in replicates, you can reduce the amount of required compute by setting `NUM_REPLICATES` in the above cell to 1.\n:::\n\nNow, we let's run the experiment:\n\n::: {#b2e6f770 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nmodel_dirs = run_trainings(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-trained model 'std_d128_rep_00' found. Returning path.\nPre-trained model 'std_d128_rep_01' found. Returning path.\nPre-trained model 'std_d128_rep_02' found. Returning path.\nPre-trained model 'std_d128_rep_03' found. Returning path.\nPre-trained model 'std_d128_rep_04' found. Returning path.\n```\n:::\n:::\n\n\n## Consistent gains across phenotypes \n\n@fig-std shows that swapping Rijal et al.’s custom choices for a canonical *embedding $\\rightarrow$ mean-pool $\\rightarrow$ linear-head* model and predicting all 18 phenotypes jointly yields boost in predictive power across the board:\n\n::: {#cell-fig-std .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nlabel_lookup = {\n    \"std\": \"Canonical\",\n}\n\n\ndef get_phenotype_r2_data(model_dir: Path) -> float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    base_arch_key, _, _ = variant_key.rpartition(\"_d\")\n    architecture_label = label_lookup[base_arch_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ncanonical_plot_data = pd.concat([get_phenotype_r2_data(model_dir) for model_dir in model_dirs])\n\n# Calculate the mean and standard error R2 over replicates\ncanonical_plot_data = (\n    canonical_plot_data.groupby([\"architecture\", \"phenotype\"])[\"r2\"]\n    .agg(mean_r2=\"mean\", std_r2=\"std\")\n    .reset_index()\n)\n\n# Concatenate the manually scraped Fig3 R2s\nrijal_r2s = fig3_results.copy(deep=True)\nrijal_r2s.drop(\"reproduction R2\", axis=1, inplace=True)\nrijal_r2s.rename(columns={\"source R2\": \"mean_r2\"}, inplace=True)\nrijal_r2s[\"std_r2\"] = 0.0\nrijal_r2s[\"architecture\"] = \"Rijal\"\ncanonical_plot_data = pd.concat([canonical_plot_data, rijal_r2s])\n\n# Plotting code.\ncanonical_plot_data[\"phenotype\"] = pd.Categorical(\n    canonical_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            canonical_plot_data[canonical_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 0.5) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Per-phenotype generalisation of multi-task vs. single-task models. Bar heights show the mean test-set \\(R^{2}\\) achieved across replicates for each phenotype, with error bars denoting the standard deviation (SD). Rijal is the single-phenotype results reported in @Rijal2025, while Canonical is our multi-output head trained to predict all phenotypes jointly.](index_files/figure-html/fig-std-output-1.png){#fig-std}\n:::\n:::\n\n\n::: {.callout-important appearance=\"simple}\n### Important caveat on the baseline we compare against\n\nThe numbers labeled \"Rijal\" in @fig-std come from the single-phenotype models reported in their Figure 3. @Rijal2025 do develop a multi-phenotype architecture, but careful inspection of [their code](https://github.com/Emergent-Behaviors-in-Biology/GenoPhenoMapAttention/blob/main/experiment/multi_env_attention_QTL_yeast_data.ipynb) shows that it still emits a scalar output per forward pass and simply loops over phenotypes at train time. In other words, each (genotype, phenotype) pair is treated as an independent sample. The predictive power of this model is shown in their Figure 4, and is actually worse than the single-phenotype models.\n\nBecause our goal is to test whether a genuine multi-output head + canonical components confer an advantage, we chose the best-performing baseline available (their single-phenotype models). The comparison is therefore conservative: any gains we report would be larger, not smaller, if the authors' scalar multi-phenotype model were used as the reference.\n:::\n\nQuantifying what changes contributed to these gains would require an ablation study, however our hunch is that the primary gains come from cross-phenotype coupling. Joint training allows the network to transfer information among pleiotropically related traits, an advantage that the single-output baseline can't exploit beyond implict relationships learned through shared attention weights.\n\nTaken together, these results validate the canonical multi-output model as a stronger starting point than the original design.\n\n# A vanilla transformer architecture\n\nThe current architecture still departs from the reference transformer in several respects: it omits residual connections, layer normalisation, and position-wise feed-forward blocks. The logical next experiment is therefore to **level up to a bona-fide vanilla transformer**, preserving the current tokenization while adding:\n\n1. **Residual (skip) connections** and **pre-LayerNorm**,  \n2. **Feed-forward sub-layers** with RELU activations,  \n3. **Scaled dot-product attention** with the canonical $1/\\sqrt{d}$ factor,  \n4. **Dropout** and **weight-decay** for regularization to prevent overfitting.\n\nIt's worth noting that transformers excel at seqence modeling--and sequences are ordered collections. While the loci in this dataset could be attributed chromosomal coordinates, and therefore in a sense an ordering, like @Rijal2025, we're treating the loci as an unordered collection. Thus, we don't add any positional encodings, either in the form of absolute or relative positional encodings.\n\n::: {#112687cf .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nNUM_REPLICATES = 5\n\njobs = []\nfor i in range(NUM_REPLICATES):\n    replicate_id = f\"{i:02d}\"\n    job_name_prefix = f\"xformer_rep_{replicate_id}\"\n\n    model_config = ModelConfig(\n        embedding_dim=256,\n        model_type=\"transformer\",\n        seq_length=1164,\n        num_layers=3,\n        nhead=4,\n        dim_feedforward=1024,\n    )\n\n    train_config = attrs.evolve(\n        train_template_config,\n        save_dir=Path(\"models/transformer\"),\n        phenotypes=phenotype_names,\n        name_prefix=job_name_prefix,\n        optimizer=\"adamw\",\n        max_epochs=80,\n    )\n\n    jobs.append((model_config, train_config))\n\nprint(f\"\\nGenerated {len(jobs)} job configurations.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nGenerated 5 job configurations.\n```\n:::\n:::\n\n\n::: {#f19ffd48 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nmodel_dirs = run_trainings(jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-trained model 'xformer_rep_00' found. Returning path.\nPre-trained model 'xformer_rep_01' found. Returning path.\nPre-trained model 'xformer_rep_02' found. Returning path.\nPre-trained model 'xformer_rep_03' found. Returning path.\nPre-trained model 'xformer_rep_04' found. Returning path.\n```\n:::\n:::\n\n\n@fig-xformer shows the change in performance:\n\n::: {#cell-fig-xformer .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nlabel_lookup = {\n    \"std\": \"Canonical\",\n    \"xformer\": \"Transformer\",\n}\n\n\ndef get_phenotype_r2_data_xformer(model_dir: Path) -> float:\n    # Determine the architecture and replicate number from model directory name\n    dir_name = model_dir.parent.parent.name  # e.g., \"std_d128_rep_09\"\n    variant_key, _, rep_str = dir_name.rpartition(\"_rep_\")\n    architecture_label = label_lookup[variant_key]\n\n    # Load and wrangle the metrics.csv. Keep only the per-phenotype test R2 values.\n    df = pd.read_csv(model_dir / \"metrics.csv\")\n    df = df[df[\"metric\"].str.startswith(\"test_r2_\")]\n    df[\"phenotype\"] = df[\"metric\"].str[8:]\n    df.drop(\"metric\", axis=1, inplace=True)\n    df.rename(columns={\"value\": \"r2\"}, inplace=True)\n    df[\"architecture\"] = architecture_label\n    df[\"replicate\"] = int(rep_str)\n    df[\"r2\"] = df[\"r2\"].astype(float)\n\n    return df\n\n\n# Concat the phenotype data across all models\ndf = pd.concat([get_phenotype_r2_data_xformer(model_dir) for model_dir in model_dirs])\ndf = df.groupby([\"architecture\", \"phenotype\"])[\"r2\"].agg(mean_r2=\"mean\", std_r2=\"std\").reset_index()\n\n# Concat with existing canonical plot data\nxformer_plot_data = pd.concat([df, canonical_plot_data])\n\n# Plotting code.\nxformer_plot_data[\"phenotype\"] = pd.Categorical(\n    xformer_plot_data[\"phenotype\"], categories=phenotype_names, ordered=True\n)\narch_order = [\"Rijal\", \"Canonical\", \"Transformer\"]\n\nx = np.arange(len(phenotype_names))\nwidth = 0.25\n\ncolor_map = {\n    \"Rijal\": apc.cloud,\n    \"Canonical\": apc.mars,\n    \"Transformer\": apc.dragon,\n}\n\nwith mpl.rc_context({\"figure.facecolor\": apc.parchment}):\n    plt.figure(dpi=300)\n\n    for i, arch in enumerate(arch_order):\n        sub = (\n            xformer_plot_data[xformer_plot_data[\"architecture\"] == arch]\n            .set_index(\"phenotype\")\n            .reindex(phenotype_names)\n        )\n        plt.bar(\n            x + (i - 1) * width,  # centre bars around tick\n            sub[\"mean_r2\"],\n            width,\n            yerr=sub[\"std_r2\"],\n            ecolor=\"#00000099\",\n            error_kw=dict(elinewidth=1.5),\n            label=arch,\n            color=color_map[arch],\n        )\n\n    plt.xticks(x, phenotype_names, rotation=90)\n    plt.xlabel(\"Phenotype\")\n    plt.ylabel(\"Mean $R^2$ (± SD)\")\n    plt.ylim(0.43, 0.69)\n    plt.legend(loc=(0.05, 0.92), ncol=3)\n    plt.tight_layout()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The vanilla transformer performances added alongside the models compared in @fig-std.](index_files/figure-html/fig-xformer-output-1.png){#fig-xformer}\n:::\n:::\n\n\nFIXME THE ABOVE PLOT IS INCOMPLETE, AND PLACEHOLDER VALUES ARE ADDED UNTIL THE MODEL FINISHES TRAINING. HOWEVER, THE FOLLOWING TEXT ACCURATELY describes the results (pretty sure).\n\nThe full transformer nudges performance above the already-improved canonical model across most phenotypes (@fig-xformer). The lift is visible but no stepwise performance gains. There's undoubtedly a lot of room for improvement: we made no attempt at hyper-parameter tuning beyond some cursory investigation of dropout and weight-decay (neither of which positively affected performance), and all other parameters were guessed zero shot.\n\n# Conclusion and outlook  \n\nOur central question was simple: *do off-the-shelf transformer components---and a genuine multi-output head---move the needle on genotype to phenotype prediction?*  \n\nThe answer is an emphatic **yes**, at least in comparison to the results in @Rijal2025. Replacing their hand-crafted components with standard ones delivers delivers significant gains for FIXME 16 of 18 yeast phenotypes (median ΔR² ≈ +0.03).\n\n## Why multi-output matters  \n\nOur findings echo a broader lesson we've learned from @Avasthi2023--that encoding multiple phenotypes jointly lets an auto-encoder predict each one more accurately than single-trait models by exploiting pleiotropic signal and non-additive structure [@Avasthi2023]. The same principle applies here: when the network can see all 18 phenotypes at once, shared genetic effects reinforce, rather than fragment, the learning signal.\n\n## A foundation, not a finish line  \n\nThere is still plenty of headroom:\n\n* No hyper-parameter search was attempted for the transformer; depth, head count, LR schedules, dropout rates and layer widths all remain untouched.\n* How far can the canonical model be pushed?\n* We didn't explore positional encodings, nor richer sequence priors (e.g. chromosomal coordinates).\n\n## An invitation to build further  \n\nAll code, configs and cached model checkpoints are available in this notebook’s [repository](https://github.com/Arcadia-Science/2025-geno-pheno-attention). The Appendix documents the engineering decisions that should help orient you to the codebase.\n\n# Appendix: a codebase primer\n\n::: {.callout-note}\nThis section elaborates on engineering details that will be of interest to those planning to reproduce, modify, or build off of this research.\n:::\n\nRather than building off the @Rijal2025 notebook files, we re-implemented their code into our own codebase to improve code quality and make room for our modifications and experimentation. Here is a very high-level summary of the changes we made:\n\n* Added configuration dataclasses to co-localize tunables\n* Saved the training/validation/test datasets to file to avoid splitting on-the-fly\n* Created PyTorch dataloaders to manage accession, batching, and data shuffling\n* Automated the training loop with [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), creating separation between the training loop and the model logic\n* Added canonical learning parameters like early stopping, learning rate scheduling, gradient norm clipping, weight decay, and more\n* Generalized the @Rijal2025 model with toggleable skip connections, layer normalization, scaled attention, dropout rate, and more\n\nThe upshot is that we're proud of this code and think it establishes a much-needed foundation that can be used to build off the research seeded by @Rijal2025.\n\n## Training models\n\nIn the analysis above, we illustrated how multiple training jobs can be run using the high level entrypoint, `run_trainings`, that:\n\n* Trains a model for a given set of phenotypes\n* Determines the best model, defined as the model with the lowest loss (MSE) calculated over the validation dataset\n* Reports the $R^2$ for the test dataset using the best model\n* Saves the model to file for downstream use\n\nThe codebase also exposes equivalent behavior through a command-line interface (CLI). It can be accessed via:\n\n```shell\n$ python -m analysis.train --help\n```\n\n## Distributed computing\n\nWe performed the analysis using Modal’s cloud infrastructure to distribute computations across GPUs, allowing us to rapidly measure performance across many different model architectures and training specifications. Whether you want to train with or without Modal can be toggled by the attribute train.use_modal. By default, Modal execution is disabled. The downside is that your training jobs will run in serial, rather than being distributed across different machines.\n\n## Caching behavior\n\nWe implemented a caching that avoids training if a model directory for a given training config already exists. We did this so that GPUs aren't a requirement for engaging with this research.\n\n* `train_config.use_cache = True` (default): Skips retraining if a model with the same configuration already exists\n* `train_config.use_cache = False`: Forces retraining regardless of existing models\n\n::: {.callout-note}\nAll training runs in this analysis use the default caching mode (`train_config.use_cache = True`), and the results are git tracked. If you execute this notebook locally, these models will be loaded from cache rather than retrained.\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}